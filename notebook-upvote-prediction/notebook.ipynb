{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f33c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import re\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2267292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. CONFIGURATION & DATA LOADING\n",
    "# ==========================================\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "SUBMISSION_PATH = \"submission.csv\"\n",
    "\n",
    "# Leakage: Metrics accumulated AFTER publication\n",
    "LEAKAGE_COLS = [\n",
    "    \"fork_count\",\n",
    "    \"views\",\n",
    "    \"downloads\",\n",
    "    \"comments_count\",\n",
    "    \"notebook_usage\",\n",
    "    \"medal\",\n",
    "    \"is_featured\",\n",
    "    \"is_trending\",\n",
    "    \"engagement_rate\",\n",
    "    \"virality_score\",\n",
    "    \"quality_score\",\n",
    "]\n",
    "\n",
    "# Irrelevant: All-null or ID metadata\n",
    "IRRELEVANT_COLS = [\n",
    "    \"usability_score\",\n",
    "    \"file_format\",\n",
    "    \"column_count\",\n",
    "    \"row_count\",\n",
    "    \"license_type\",\n",
    "    \"content_type\",\n",
    "    \"author_username\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_and_filter(path, is_train=True):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {path}\")\n",
    "        return None\n",
    "\n",
    "    # Filter for notebooks only (as per problem description)\n",
    "    if \"content_type\" in df.columns:\n",
    "        df = df[df[\"content_type\"] == \"notebook\"].copy()\n",
    "\n",
    "    # Drop prohibited columns\n",
    "    cols_to_drop = [c for c in LEAKAGE_COLS + IRRELEVANT_COLS if c in df.columns]\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load Data\n",
    "print(\"Loading data...\")\n",
    "train_df = load_and_filter(TRAIN_PATH, is_train=True)\n",
    "test_df = load_and_filter(TEST_PATH, is_train=False)\n",
    "\n",
    "if train_df is None or test_df is None:\n",
    "    exit()\n",
    "\n",
    "# Target Transformation: log(1 + y)\n",
    "# This matches the evaluation metric: MAE of logs\n",
    "y = np.log1p(train_df[\"upvotes\"])\n",
    "X = train_df.drop(columns=[\"upvotes\"])\n",
    "X_test = test_df.copy()\n",
    "\n",
    "# Store IDs for submission\n",
    "test_ids = X_test[\"content_id\"]\n",
    "X = X.drop(columns=[\"content_id\"])\n",
    "X_test = X_test.drop(columns=[\"content_id\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3430c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# --- A. Date Features ---\n",
    "def process_dates(df):\n",
    "    # Handle dd/mm/yyyy description vs ISO snippet. dayfirst=True handles the text description correctly.\n",
    "    # Coerce errors allows the code to proceed even if some dates are malformed.\n",
    "    for col in [\"created_date\", \"last_updated\"]:\n",
    "        if col in df.columns:\n",
    "            # Convert to datetime\n",
    "            df[col] = pd.to_datetime(df[col], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "            # Feature 1: Numerical Timestamp (Days since Epoch)\n",
    "            # This preserves the linear progression of time better than raw Year/Month\n",
    "            df[f\"{col}_ts\"] = df[col].astype(np.int64) // 10**9 // 86400\n",
    "\n",
    "            # Feature 2: Day of Week (Cyclical pattern)\n",
    "            df[f\"{col}_dow\"] = df[col].dt.dayofweek\n",
    "\n",
    "            # Drop original date object\n",
    "            df = df.drop(columns=[col])\n",
    "    return df\n",
    "\n",
    "\n",
    "X = process_dates(X)\n",
    "X_test = process_dates(X_test)\n",
    "\n",
    "# --- B. Interaction Terms ---\n",
    "# Update Frequency: How often is it updated? (Age / Update Count)\n",
    "# Add +1 to denominator to avoid division by zero\n",
    "if \"days_since_creation\" in X.columns and \"update_count\" in X.columns:\n",
    "    X[\"update_freq\"] = X[\"days_since_creation\"] / (X[\"update_count\"] + 1)\n",
    "    X_test[\"update_freq\"] = X_test[\"days_since_creation\"] / (X_test[\"update_count\"] + 1)\n",
    "\n",
    "# --- C. Text Features (Title) ---\n",
    "# Use TF-IDF to capture keywords (e.g., \"Tutorial\", \"Guide\", \"Titanic\")\n",
    "print(\"Processing text...\")\n",
    "tfidf = TfidfVectorizer(max_features=100, stop_words=\"english\")\n",
    "title_train = tfidf.fit_transform(X[\"title\"].fillna(\"\"))\n",
    "title_test = tfidf.transform(X_test[\"title\"].fillna(\"\"))\n",
    "\n",
    "# Create DataFrames for text features\n",
    "title_cols = [f\"title_{i}\" for i in range(title_train.shape[1])]\n",
    "title_train_df = pd.DataFrame(title_train.toarray(), columns=title_cols, index=X.index)\n",
    "title_test_df = pd.DataFrame(title_test.toarray(), columns=title_cols, index=X_test.index)\n",
    "\n",
    "X = X.drop(columns=[\"title\"])\n",
    "X_test = X_test.drop(columns=[\"title\"])\n",
    "\n",
    "\n",
    "# --- D. Multi-Label Features (Topics & Libraries) ---\n",
    "def process_multilabel(train, test, col_name, top_n=30):\n",
    "    # 1. Split strings by pipe '|'\n",
    "    train_split = train[col_name].fillna(\"\").astype(str).apply(lambda x: x.split(\"|\") if x else [])\n",
    "    test_split = test[col_name].fillna(\"\").astype(str).apply(lambda x: x.split(\"|\") if x else [])\n",
    "\n",
    "    # 2. Binarize\n",
    "    mlb = MultiLabelBinarizer(sparse_output=False)\n",
    "    mlb.fit(train_split)\n",
    "\n",
    "    # Transform\n",
    "    train_enc = pd.DataFrame(\n",
    "        mlb.transform(train_split),\n",
    "        columns=[f\"{col_name}_{c}\" for c in mlb.classes_],\n",
    "        index=train.index,\n",
    "    )\n",
    "    test_enc = pd.DataFrame(\n",
    "        mlb.transform(test_split),\n",
    "        columns=[f\"{col_name}_{c}\" for c in mlb.classes_],\n",
    "        index=test.index,\n",
    "    )\n",
    "\n",
    "    # 3. Keep only Top N most frequent to reduce noise/memory\n",
    "    if train_enc.shape[1] > top_n:\n",
    "        top_cols = train_enc.sum().sort_values(ascending=False).head(top_n).index\n",
    "        return train_enc[top_cols], test_enc[top_cols]\n",
    "\n",
    "    return train_enc, test_enc\n",
    "\n",
    "\n",
    "print(\"Processing tags...\")\n",
    "# Libraries\n",
    "libs_train, libs_test = process_multilabel(X, X_test, \"libraries_used\", top_n=50)\n",
    "X = X.drop(columns=[\"libraries_used\"])\n",
    "X_test = X_test.drop(columns=[\"libraries_used\"])\n",
    "\n",
    "# Topics\n",
    "topics_train, topics_test = process_multilabel(X, X_test, \"all_topics\", top_n=30)\n",
    "X = X.drop(columns=[\"all_topics\"])\n",
    "X_test = X_test.drop(columns=[\"all_topics\"])\n",
    "\n",
    "# --- E. Categorical Encoding ---\n",
    "# Author Tier is ordinal\n",
    "tier_mapping = {\"Novice\": 0, \"Contributor\": 1, \"Expert\": 2, \"Master\": 3, \"Grandmaster\": 4}\n",
    "X[\"author_tier\"] = X[\"author_tier\"].map(tier_mapping)\n",
    "X_test[\"author_tier\"] = X_test[\"author_tier\"].map(tier_mapping)\n",
    "\n",
    "# OneHotEncode others\n",
    "cat_cols = [\"programming_language\", \"primary_topic\"]\n",
    "# Note: We dropped author_username due to high cardinality/irrelevance in this simplified model\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "# Fill missing categoricals\n",
    "X[cat_cols] = X[cat_cols].fillna(\"Missing\")\n",
    "X_test[cat_cols] = X_test[cat_cols].fillna(\"Missing\")\n",
    "\n",
    "ohe_train = pd.DataFrame(\n",
    "    ohe.fit_transform(X[cat_cols]), columns=ohe.get_feature_names_out(), index=X.index\n",
    ")\n",
    "ohe_test = pd.DataFrame(\n",
    "    ohe.transform(X_test[cat_cols]), columns=ohe.get_feature_names_out(), index=X_test.index\n",
    ")\n",
    "\n",
    "X = X.drop(columns=cat_cols)\n",
    "X_test = X_test.drop(columns=cat_cols)\n",
    "\n",
    "# --- F. Numerical Imputation ---\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_num = pd.DataFrame(imputer.fit_transform(X), columns=X.columns, index=X.index)\n",
    "X_test_num = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Assemble final dataset\n",
    "X_final = pd.concat([X_num, title_train_df, libs_train, topics_train, ohe_train], axis=1)\n",
    "X_test_final = pd.concat([X_test_num, title_test_df, libs_test, topics_test, ohe_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50165a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. MODEL TRAINING (Hyperparameter Search)\n",
    "# ==========================================\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:absoluteerror\",  # Optimization for MAE\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\",  # Faster training\n",
    ")\n",
    "\n",
    "# Search Space\n",
    "param_dist = {\n",
    "    \"n_estimators\": [100, 300, 500],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Randomized Search\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=xgb_reg,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "search.fit(X_final, y)\n",
    "\n",
    "print(f\"Best Log-MAE: {-search.best_score_:.4f}\")\n",
    "print(f\"Best Params: {search.best_params_}\")\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# ==========================================\n",
    "# 4. PREDICTION & SUBMISSION\n",
    "# ==========================================\n",
    "\n",
    "# Predict log(1+y)\n",
    "log_preds = best_model.predict(X_test_final)\n",
    "\n",
    "# Inverse Transform: exp(pred) - 1\n",
    "preds = np.expm1(log_preds)\n",
    "\n",
    "# Clip: Upvotes cannot be negative\n",
    "preds = np.maximum(preds, 0)\n",
    "\n",
    "submission = pd.DataFrame({\"content_id\": test_ids, \"upvotes\": preds})\n",
    "\n",
    "submission.to_csv(SUBMISSION_PATH, index=False)\n",
    "print(f\"Submission saved to {SUBMISSION_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shipd-eris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
